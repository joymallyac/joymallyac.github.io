<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>Joymallya Chakraborty</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Joymallya Chakraborty</name>
              </p>
              <p>I am an Applied Scientist II at <a href="https://www.amazon.science/"> Amazon </a> in Seattle. I completed my Ph.D. in Computer Science from <a href="https://www.ncsu.edu/"> North Carolina State University</a>, under the supervision of Dr. <a href="http://menzies.us/"> Tim Menzies</a>. My research interests include algorithmic bias, ML model optimization, interpretability & explanation of black-box ML models. 
              </p>
              <p>Before coming to NC State, I was a full-stack software developer at <a href="https://www.tcg-digital.com/"> TCG Digital</a>. I obtained my bachelor's degree in Computer Science from <a href="http://www.jaduniv.edu.in/"> Jadavpur University</a>. I 
              	have done research internships at <a href="https://www.intel.com/content/www/us/en/homepage.html"> Intel Corporation </a> (Bellevue, WA),  <a href="http://www.research.ibm.com/labs/watson/"> IBM T.J. Watson Research Labs</a>  (Yorktown Heights,  NY), & <a href="https://www.amazon.science/"> Amazon </a> (Seattle, WA). </p>
              <p align=center>
                <a href="mailto:chkrjoymallya@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Joymallya_Resume_After_PhD.pdf">CV</a> &nbsp/&nbsp               
                <a href="https://scholar.google.co.in/citations?hl=en&pli=1&user=Q9y62SEAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/joymallya-chakraborty-06b7bba7/"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://github.com/joymallyac"> GitHub </a> &nbsp/&nbsp
                <a href="https://twitter.com/chkrjoymallya"> Twitter </a>
              </p>
            </td>
            <td width="33%">
              <img src="images/Joy_Circle.png" width = "250" height="250">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                My research mainly focuses on solving real world problems in the software engineering field using data mining and artificial intelligence methods. Previously I worked on finding "discrimination" in social coding platform. Currently my research focus is on finding and mitigating algorithmic "bias" in machine learning models.
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Publication</heading>
            </td>
          </tr>
        </table>

         <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/FairMask.jpg" alt="prl" width="145" height="200"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://ieeexplore.ieee.org/abstract/document/9951398">
                  <papertitle>FairMask: Better Fairness via Model-based Rebalancing of Protected Attributes</papertitle>
                </a></p>       
                Kewen Peng, <strong>Joymallya Chakraborty</strong>, Tim Menzies
                <br>  <strong> IEEE Transactions on Software Engineering (TSE) 2022</strong>            
                  <p>FairMask is a model-based extrapolation method that is capable of both mitigating bias and explaining the cause behind it. In our FairMask approach, protected attributes are represented by models learned from the other independent variables (and these models offer extrapolations over the space between existing examples). We then use the extrapolation models to relabel protected attributes later seen in testing data or deployment time. Our approach aims to offset the biased predictions of the classification model by rebalancing the distribution of protected attributes. FairMask can achieve significantly better group and individual fairness (as measured in different metrics) than benchmark methods.
                </p>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/fairness_metrics.png" alt="prl" width="145" height="200"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://arxiv.org/abs/2110.13029">
                  <papertitle>Fair Enough: Searching for Sufficient Measures of Fairness</papertitle>
                </a></p>                
                Suvodeep Majumder, <strong>Joymallya Chakraborty</strong>, Gina R Bai, Kathryn T Stolee, Tim Menzies
                <br>  <strong> ACM Transactions on Software Engineering and Methodology (TOSEM) 2022 </strong>            
                  <p> Testing machine learning software for ethical bias has become a pressing current concern. In response, recent research has proposed a plethora of new fairness metrics, for example, the dozens of fairness metrics in the IBM AIF360 toolkit. This raises the question: How can any fairness tool satisfy such a diverse range of goals? This paper shows that many of those fairness metrics effectively measure the same thing. Based on experiments using seven real-world datasets, we find that (a) 26 classification metrics can be clustered into seven groups, and (b) four dataset metrics can be clustered into three groups. Further, each reduced set may actually predict different things. Hence, it is no longer necessary (or even possible) to satisfy all fairness metrics.
                </p>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/SSL.png" alt="prl" width="145" height="200"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://dl.acm.org/doi/abs/10.1145/3524491.3527305">
                  <papertitle>Fair-SSL: Building fair ML Software with less data</papertitle>
                </a></p>                
                <strong> Joymallya Chakraborty</strong>, Suvodeep Majumder, Huy Tu
                <br>  <strong> ICSE 2022 </strong>  (Fairware)            
                  <p> Semi-supervised learning is a machine learning technique where, incrementally, labeled data is used to generate pseudo-labels for the rest of data (and then all that data is used for model training). In this work, we apply four popular semi-supervised techniques as pseudo-labelers to create fair classification models. Our framework, Fair-SSL, takes a very small amount (10\%) of labeled data as input and generates pseudo-labels for the unlabeled data. To the best of our knowledge, this is the first SE work where semi-supervised techniques are used to fight against ethical bias in SE ML models.
                </p>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/ACM_AWARD.png" alt="prl" width="145" height="200"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://dl.acm.org/doi/10.1145/3468264.3468537">
                  <papertitle>Bias in Machine Learning Software: Why? How? What to do?</papertitle>
                </a></p>                
                <strong> Joymallya Chakraborty</strong>, Suvodeep Majumder, Tim Menzies
                <br>  <strong> ESEC/FSE 2021 (ACM SIGSOFT Distinguished Paper Award Winner) </strong>            
                  <p> This paper postulates that the root causes of bias are the prior decisions that affect- (a) what data was selected and (b) the labels assigned to those examples. Our Fair-SMOTE algorithm removes biased labels; and rebalances internal distributions such that based on sensitive attribute,  examples are equal in both positive and negative classes. On testing, it was seen that this method was just as effective at reducing bias as prior approaches. Further, models generated via Fair-SMOTE achieve higher performance (measured in terms of recall and F1) than other state-of-the-art fairness improvement algorithms.
                </p>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/Fair-knn.png" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://ieeexplore.ieee.org/document/9286091">
                  <papertitle>Making Fair ML Software using Trustworthy Explanation</papertitle>
                </a></p>                
                <strong> Joymallya Chakraborty</strong>, Kewen Peng, Tim Menzies
                <br>  <strong> ASE 2020 </strong> (NIER)              
                  <p> Machine learning software is being used in many applications (finance, hiring, admissions, criminal justice) having a huge social impact. But sometimes the behavior of this software is biased and it shows discrimination based on some sensitive attributes such as sex, race, etc. Prior works concentrated on finding and mitigating bias in ML models. A recent trend is using instance-based model-agnostic explanation methods such as LIME to find out bias in the model prediction. Our work concentrates on finding shortcomings of current bias measures and explanation methods. We show how our proposed method based on K nearest neighbors can overcome those shortcomings and find the underlying bias of black-box models. Our results are more trustworthy and helpful for the practitioners. Finally, We describe our future framework combining explanation and planning to build fair software.
                </p>
              </p>
            </td>
          </tr>
        </table>

        
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/Fairway.png" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://dl.acm.org/doi/abs/10.1145/3368089.3409697">
                  <papertitle>Fairway: A way to build fair ML Software</papertitle>
                </a></p>                
                <strong> Joymallya Chakraborty</strong>, Suvodeep Majumder, Zhe Yu, Tim Menzies
                <br>  <strong> ESEC/FSE 2020  </strong>               
                  <p> Machine learning software is increasingly being used to make decisions that affect people's lives. But sometimes, the core part of this software (the learned model), behaves in a biased manner that gives undue advantages to a specific group of people (determined by sex, race, etc.). In this work, we a)explain how ground-truth bias in training data affects machine learning model fairness and how to find that bias in AI software, b)propose a methodFairwaywhich combines pre-processing and in-processing approach to remove ethical bias from training data and trained model. Our results show that we can find bias and mitigate bias in a learned model, without much damaging the predictive performance of that model. We propose that (1) testing for bias and (2) bias mitigation should be a routine part of the machine learning software development life cycle. Fairway offers much support for these two purposes.
                </p>
              </p>
            </td>
          </tr>
        </table>      
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/Bias.jpg" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://arxiv.org/abs/1905.05786">
                  <papertitle>Software Engineering for Fairness: A Case Study with Hyperparameter Optimization</papertitle>
                </a></p>                
                <strong> Joymallya Chakraborty</strong>, Tianpei Xia, Fahmid M. Fahid, Tim Menzies
                <br>  <strong> ASE 2019  </strong> (LBR Workshop)               
                  <p> Machine learning software is increasingly being used to make decisions that affect people's lives.  Potentially,  the application of that software will result in fairer decisions because (unlike humans) machine learning software is not biased. However, recent results show that the software within many data mining packages exhibit "group discrimination"; i.e. their decisions are  inappropriately affected by  "protected attributes" (e.g., race, gender, age, etc.). This paper shows that making <strong> fairness </strong> as a goal during hyperparameter optimization can preserve the predictive power of a model learned from a data miner while also generates fairer results. To the best of  our knowledge, this is the first application  of hyperparameter optimization as a tool for software engineers to generate fairer software.
                </p>
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/LN.jpg" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://dl.acm.org/doi/abs/10.1145/3338906.3340450">
                  <papertitle>Predicting Breakdowns in Cloud Services (with SPIKE)</papertitle>
                </a></p>                
                Jianfeng Chen, <strong> Joymallya Chakraborty</strong>, Tim Menzies, Philip Clark, Kevin Haverlock, Snehit  Cherian
                <br>  <strong> ESEC/FSE 2019  </strong>              
                  <p> Maintaining  web-services  is a mission-critical task. Any downtime of web-based  services means loss of revenue.  Worse, such down times can damage the reputation of an organization as a reliable service provider (and in the current competitive web services market, such a loss of reputation causes extensive loss of future revenue). To address this issue, we developed <strong> SPIKE </strong>,  a data mining tool which can predict upcoming service breakdowns,  half an hour into the future.
                </p>
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/social_model.png" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://arxiv.org/abs/1904.09954">
                  <papertitle>Why Software Projects need Heroes</papertitle>
                </a></p>                
                Suvodeep Majumder, <strong> Joymallya Chakraborty</strong>, Amritanshu Agrawal, Tim Menzies
                <br>                
                  <p> A "hero" project is one where  80% or more of the contributions are made by the 20% of the developers. In the literature, such 	
                  	projects are  deprecated since they might   cause bottlenecks 
					in development and communication. This paper explores the effect of having heroes in a project, from a code quality perspective. 
					After experimenting on <strong> 1100+ </strong> GitHub projects, we conclude that heroes are a very useful part of modern open source projects.
                </p>
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/Gender_Bias.png" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://ai.google/research/pubs/pub47860">
                  <papertitle>Measuring the Effects of Gender Bias on GitHub</papertitle>
                </a></p>                
                Nasif Imtiaz, Justin Middleton, <strong> Joymallya Chakraborty</strong>, Neill Robson, Gina Bai, Emerson Murphy-Hill
                <br>  <strong> ICSE 2019  </strong>               
                  <p> Diversity, including gender diversity, is valued by many software development organizations, yet the field remains dominated by men. One reason for this lack of diversity is gender bias. In this paper, we study the effects of that bias by using an existing framework derived from the gender studies literature. We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub, then evaluate those hypotheses quantitatively. While our results show that effects are largely invisible on the GitHub platform itself, there are still signals of women concentrating their work in fewer places and being more restrained in communication than men.
                </p>
              </p>
            </td>
          </tr>
        </table>
        
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/Spanning_Tree.png" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://link.springer.com/article/10.1007/s40747-018-0079-7">
                  <papertitle>Algorithms for generating all possible spanning trees of a simple undirected connected graph: an extensive review</papertitle>
                </a></p>                
                Maumita Chakraborty, Sumon Chowdhury, <strong>Joymallya Chakraborty</strong>, Ranjan Mehera, Rajat Kumar Pal
                <br>  <strong> Complex & Intelligent Systems (Springer),2018  </strong>               
                  <p> Generation of all possible spanning trees of a graph is a major area of research in graph theory as the number of spanning trees of a graph increases exponentially with graph size. Several algorithms of varying efficiency have been developed since early 1960s by researchers around the globe. This article is an exhaustive literature survey on these algorithms, assuming the input to be a simple undirected connected graph of finite order, and contains detailed analysis and comparisons in both theoretical and experimental behavior of these algorithms.
                </p>
              </p>
            </td>
          </tr>
        </table>

        

         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Industrial Experience</heading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/amazon.png" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://www.amazon.science/">
                  <papertitle> Applied Scientist Intern </papertitle>
                </a></p>
                <br> May 2021 - August 2021 (Seattle,WA)                
                  <p> I worked as a member of the Abuse Prevention Science team to find emerging abuse on Amazon Product Detail Page. 
                </p>
              </p> 
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/ibm-research-logo.jpg" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="http://www.research.ibm.com/labs/watson/">
                  <papertitle>Doctorate Research Intern </papertitle>
                </a></p>
                <br> June 2020 - August 2020 (Yorktown Heights,NY)                
                  <p> I worked on State Management & Persistence in a project called <a href="https://www.ibm.com/cloud/blog/announcements/ibm-mono2micro"> Mono2Micro </a> which converts Monolith applications to Microservices
                </p>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/Intel.png" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://www.intel.in/content/www/in/en/homepage.html">
                  <papertitle>Software Engineer Research Intern</papertitle>
                </a></p>
                <br> May 2019 - August 2019 (Bellevue,Seattle)                
                  <p> I worked on post-training quantization of ONNX, Tensorflow DL models and Computational Graph Optimization on Onnxruntime.
                </p>
              </p>
              <p>
                </a></p>
                <br> May 2018 - August 2018 (Bellevue,Seattle)                
                  <p> I explored optimization opportunities of .NET Core Garbage Collection and implemented PoC (Proof of Concept) prototypes. The prototypes were then verified against different workloads.
                </p>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/TCG_Digital.png" alt="prl" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <a href="https://www.tcg-digital.com/">
                  <papertitle>Software Developer</papertitle>
                </a></p>
                <br> July 2015 - June 2017 (Salt Lake,Kolkata)                
                  <p> I was a core developer for two different projects. I designed and developed a B2B Travel Search Engine. I was responsible for implementing middleware services and integrating those with the front end. For the second project, I designed & implemented an intelligence software to retrieve, analyze, transform and report data for business intelligence. It allows users to create different dashboards using its own customizable visualization. It also features advanced analytics concepts like data modelling, forecasting, & determining product affinity.
                </p>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Talks</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
        	<tr>
            <td width="25%"><img src="images/ASE-2019.jpg" alt="pacman" width="160" height="260"></td>
            <td width="75%" valign="center">
              <p>
                <a href="https://esec-fse19.ut.ee">
                  <papertitle>ASE 2019</papertitle>
                </a></p>                                
                  <p> I attended the 34th IEEE/ACM International Conference on <strong><a href="https://2019.ase-conferences.org/"> Automated Software Engineering (ASE 2019) </a></strong> in San Diego, California. I presented a poster on the <strong><a href="https://2019.ase-conferences.org/track/ase-2019-Late-Breaking-Results?track=ASE%20Late%20Breaking%20Results"> Late Breaking Results </a></strong> Section. The poster is about the short paper I submitted <strong><a href="https://arxiv.org/abs/1905.05786"> Software Engineering for Fairness: A Case Study with Hyperparameter Optimization </a></strong>.
                </p>
              </p>
            </td>
          </tr>
          <tr>
            <td width="25%"><img src="images/ESEC_FSE.jpg" alt="pacman" width="160" height="260"></td>
            <td width="75%" valign="center">
              <p>
                <a href="https://esec-fse19.ut.ee">
                  <papertitle>ESEC/FSE 2019</papertitle>
                </a></p>                                
                  <p> I attended the 27th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering <strong><a href="https://esec-fse19.ut.ee" >ESEC/FSE 2019 </a></strong> in Tallinn, Estonia and presented two papers there. The first paper <strong><a href="https://arxiv.org/abs/1905.07019" >TERMINATOR: Better Automated UI Test Case Prioritization </a></strong> is related to Testcase prioritization  and second paper <strong><a href="https://arxiv.org/abs/1905.06390" >Predicting Breakdowns in Cloud Services (with SPIKE) </a></strong>  is related to Cloud Computing.
                </p>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>TA Experience</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/pacman.jpg" alt="pacman" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>
                <a >
                  <papertitle>CSC230 - Fall 2017 (C and Software Tools)</papertitle>
                </a>
                <br>
                <br>
                <a >
                  <papertitle>CSC326 - Spring 2018 (Software Engineering)</papertitle>
                </a>
                <br>
                <br>
                <a >
                  <papertitle>CSC520 - Fall 2018 (Artificial Intelligence)</papertitle>
                </a>
                <br>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Professional Service</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/reviewer.png" alt="pacman" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>
                <a >
                  <papertitle>Empirical Software Engineering, 2021 (Reviewer)</papertitle>
                </a>
                <br>
                <br>
                <a >
                  <papertitle>IEEE Transactions on Software Engineering, 2021 (Reviewer)</papertitle>
                </a>
                <br>
              </p>
            </td>
          </tr>
        </table>

        <hr noshade>
        <div >
        	<div align="center"> <a href="https://clustrmaps.com/site/1asb5" title="Visit tracker"><img src="https://www.clustrmaps.com/map_v2.png?d=In7w9vnUaam1Bikfa9TateAkSm3oGsdBFKt-0jvmDBM&cl=ffffff"></a></div>        	
	  		<div text-align="center">
	    	<font size="2" text-align="center"><a href="https://people.eecs.berkeley.edu/~barron/"> Website Template Credits </a>Last updated: 05/14/2019	</font>
	  	</div>
		</div>
        </td>
    </tr>
  </table>           
</body>

</html>
